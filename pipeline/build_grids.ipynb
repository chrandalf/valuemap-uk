{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07cd939d",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "title": "[code]"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded 5,100.3 MB\n",
      "Done: pp-2025.txt\n"
     ]
    }
   ],
   "source": [
    "import os, time, requests\n",
    "URL = \"http://prod.publicdata.landregistry.gov.uk.s3-website-eu-west-1.amazonaws.com/pp-complete.txt\"\n",
    "##URL = \"https://s3.eu-west-1.amazonaws.com/prod1.publicdata.landregistry.gov.uk/pp-2025.txt\"\n",
    "OUT = \"pp-2025.txt\"\n",
    "\n",
    "def download_resume(url, out_path, retries=20, chunk_size=1024*1024):\n",
    "    s = requests.Session()\n",
    "    headers = {\"User-Agent\": \"Mozilla/5.0\", \"Accept-Encoding\": \"identity\"}\n",
    "    downloaded = os.path.getsize(out_path) if os.path.exists(out_path) else 0\n",
    "\n",
    "    for attempt in range(1, retries + 1):\n",
    "        try:\n",
    "            h = headers.copy()\n",
    "            if downloaded:\n",
    "                h[\"Range\"] = f\"bytes={downloaded}-\"\n",
    "\n",
    "            with s.get(url, stream=True, headers=h, timeout=(30, 300)) as r:\n",
    "                if r.status_code == 416:\n",
    "                    print(\"Already complete.\")\n",
    "                    return\n",
    "                r.raise_for_status()\n",
    "\n",
    "                mode = \"ab\" if downloaded else \"wb\"\n",
    "                with open(out_path, mode) as f:\n",
    "                    last = time.time()\n",
    "                    for chunk in r.iter_content(chunk_size=chunk_size):\n",
    "                        if not chunk:\n",
    "                            continue\n",
    "                        f.write(chunk)\n",
    "                        downloaded += len(chunk)\n",
    "                        if time.time() - last > 2:\n",
    "                            print(f\"\\rDownloaded {downloaded/1e6:,.1f} MB\", end=\"\")\n",
    "                            last = time.time()\n",
    "\n",
    "            print(f\"\\nDone: {out_path}\")\n",
    "            return\n",
    "\n",
    "        except (requests.Timeout, requests.ConnectionError) as e:\n",
    "            wait = min(2 ** attempt, 60)\n",
    "            print(f\"\\nAttempt {attempt} failed: {e} | retrying in {wait}s (resume at {downloaded/1e6:,.1f} MB)\")\n",
    "            time.sleep(wait)\n",
    "\n",
    "    raise RuntimeError(\"Failed after retries\")\n",
    "\n",
    "download_resume(URL, OUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6dfec170",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "title": "[code]"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspaces/valuemap-uk/pipeline\n",
      "['pp-2025.txt', 'build_grids.ipynb', 'Untitled.ipynb', 'build_grids.py', '.ipynb_checkpoints']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())\n",
    "print(os.listdir(\".\")[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4a4cb6ce-2b40-493c-85b9-3bb51f92b3d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "ROOT = Path.cwd()   # start here\n",
    "# if you're inside notebooks/, go up one:\n",
    "if (ROOT / \"package.json\").exists() is False and (ROOT.parent / \"package.json\").exists():\n",
    "    ROOT = ROOT.parent\n",
    "\n",
    "DATA = ROOT / \"data\"\n",
    "DATA.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e9342617-8d91-41da-8d84-b3c80ab75c10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CWD: /workspaces/valuemap-uk/pipeline\n",
      "Here: ['pp-2025.txt', 'build_grids.ipynb', 'Untitled.ipynb', 'build_grids.py', '.ipynb_checkpoints']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(\"CWD:\", os.getcwd())\n",
    "print(\"Here:\", os.listdir(\".\")[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "39b71aba",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "title": "[code]"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "for d in os.listdir(\"data\"):\n",
    "    p = f\"data/{d}\"\n",
    "    print(\"\\n==\", p)\n",
    "    print(os.listdir(p)[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "97a864b5",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "title": "[code]"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows kept: 4705970\n",
      "Date range: 2021-02-06 -> 2025-12-24\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "path = \"pp-2025.txt\"\n",
    "\n",
    "# Full PPD schema (16 columns)\n",
    "all_cols = [\n",
    "    \"transaction_id\", \"price\", \"date\", \"postcode\",\n",
    "    \"property_type\", \"new_build\", \"tenure\",\n",
    "    \"paon\", \"saon\", \"street\", \"locality\", \"town_city\",\n",
    "    \"district\", \"county\", \"ppd_category\", \"record_status\"\n",
    "]\n",
    "\n",
    "usecols = [\"transaction_id\", \"price\", \"date\", \"postcode\", \"property_type\", \"new_build\", \"record_status\"]\n",
    "\n",
    "cutoff = (pd.Timestamp.today().normalize() - pd.DateOffset(years=5)).date()\n",
    "\n",
    "chunks = []\n",
    "for chunk in pd.read_csv(\n",
    "    path,\n",
    "    header=None,\n",
    "    names=all_cols,\n",
    "    usecols=usecols,\n",
    "    dtype={\n",
    "        \"transaction_id\": \"string\",\n",
    "        \"postcode\": \"string\",\n",
    "        \"property_type\": \"string\",\n",
    "        \"new_build\": \"string\",\n",
    "        \"record_status\": \"string\",\n",
    "    },\n",
    "    chunksize=500_000\n",
    "):\n",
    "    # Parse dates for this chunk\n",
    "    chunk[\"date\"] = pd.to_datetime(chunk[\"date\"], errors=\"coerce\")\n",
    "    chunk = chunk[chunk[\"date\"].notna()]\n",
    "\n",
    "    # Keep only \"A\" records (usual rule)\n",
    "    chunk = chunk[chunk[\"record_status\"] == \"A\"]\n",
    "\n",
    "    # Last 5 years\n",
    "    chunk = chunk[chunk[\"date\"].dt.date >= cutoff]\n",
    "\n",
    "    chunks.append(chunk)\n",
    "\n",
    "df = pd.concat(chunks, ignore_index=True)\n",
    "\n",
    "# Month-start timestamp (for your downstream logic)\n",
    "df[\"month\"] = df[\"date\"].dt.to_period(\"M\").dt.to_timestamp()\n",
    "\n",
    "print(\"Rows kept:\", len(df))\n",
    "print(\"Date range:\", df[\"date\"].min().date(), \"->\", df[\"date\"].max().date())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e66f2baa",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "title": "[code]"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>transaction_id</th>\n",
       "      <th>price</th>\n",
       "      <th>date</th>\n",
       "      <th>postcode</th>\n",
       "      <th>property_type</th>\n",
       "      <th>new_build</th>\n",
       "      <th>record_status</th>\n",
       "      <th>month</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{D707E535-5720-0AD9-E053-6B04A8C067CC}</td>\n",
       "      <td>260000</td>\n",
       "      <td>2021-08-06</td>\n",
       "      <td>SO45 2HT</td>\n",
       "      <td>T</td>\n",
       "      <td>N</td>\n",
       "      <td>A</td>\n",
       "      <td>2021-08-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{D707E535-5721-0AD9-E053-6B04A8C067CC}</td>\n",
       "      <td>375000</td>\n",
       "      <td>2021-09-01</td>\n",
       "      <td>SO23 7FR</td>\n",
       "      <td>S</td>\n",
       "      <td>N</td>\n",
       "      <td>A</td>\n",
       "      <td>2021-09-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{D707E535-5723-0AD9-E053-6B04A8C067CC}</td>\n",
       "      <td>132000</td>\n",
       "      <td>2021-06-28</td>\n",
       "      <td>SP11 6RL</td>\n",
       "      <td>F</td>\n",
       "      <td>N</td>\n",
       "      <td>A</td>\n",
       "      <td>2021-06-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{D707E535-5724-0AD9-E053-6B04A8C067CC}</td>\n",
       "      <td>295000</td>\n",
       "      <td>2021-09-10</td>\n",
       "      <td>SP11 6TU</td>\n",
       "      <td>T</td>\n",
       "      <td>N</td>\n",
       "      <td>A</td>\n",
       "      <td>2021-09-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{D707E535-5725-0AD9-E053-6B04A8C067CC}</td>\n",
       "      <td>360000</td>\n",
       "      <td>2021-08-27</td>\n",
       "      <td>SO51 0AX</td>\n",
       "      <td>T</td>\n",
       "      <td>N</td>\n",
       "      <td>A</td>\n",
       "      <td>2021-08-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           transaction_id   price       date  postcode  \\\n",
       "0  {D707E535-5720-0AD9-E053-6B04A8C067CC}  260000 2021-08-06  SO45 2HT   \n",
       "1  {D707E535-5721-0AD9-E053-6B04A8C067CC}  375000 2021-09-01  SO23 7FR   \n",
       "2  {D707E535-5723-0AD9-E053-6B04A8C067CC}  132000 2021-06-28  SP11 6RL   \n",
       "3  {D707E535-5724-0AD9-E053-6B04A8C067CC}  295000 2021-09-10  SP11 6TU   \n",
       "4  {D707E535-5725-0AD9-E053-6B04A8C067CC}  360000 2021-08-27  SO51 0AX   \n",
       "\n",
       "  property_type new_build record_status      month  \n",
       "0             T         N             A 2021-08-01  \n",
       "1             S         N             A 2021-09-01  \n",
       "2             F         N             A 2021-06-01  \n",
       "3             T         N             A 2021-09-01  \n",
       "4             T         N             A 2021-08-01  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e59e504f",
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2026-02-05T10:52:58.354625Z",
     "iopub.status.busy": "2026-02-05T10:52:58.354160Z",
     "iopub.status.idle": "2026-02-05T10:53:27.601785Z",
     "shell.execute_reply": "2026-02-05T10:53:27.600843Z",
     "shell.execute_reply.started": "2026-02-05T10:52:58.354582Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "title": "[code]"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "path_east_north = \"/kaggle/input/postcode-eastnorth/ONSPD_Online_latest_Postcode_Centroids_.csv\"\n",
    "\n",
    "cols = [\n",
    "    \"x\",\"y\",\"PCD7\",\"PCD8\",\"PCDS\",\"DOINTR\",\"DOTERM\",\n",
    "    \"EAST1M\",\"NORTH1M\"\n",
    "]\n",
    "\n",
    "df_en = pd.read_csv(\n",
    "    path_east_north,    \n",
    "    names=cols,\n",
    "    header=None,\n",
    "    usecols=cols,\n",
    "    skiprows=1,\n",
    "    dtype={\n",
    "        \"x\": \"string\",\n",
    "        \"y\": \"string\",\n",
    "        \"PCD7\": \"string\",\n",
    "        \"PCD8\": \"string\",\n",
    "        \"PCDS\": \"string\",\n",
    "        \"DOINTR\": \"string\",\n",
    "        \"DOTERM\": \"string\",\n",
    "        \"EAST1M\": \"string\",\n",
    "        \"NORTH1M\": \"string\",        \n",
    "    }\n",
    ")\n",
    "\n",
    "df_en.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5430c038",
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2026-02-05T10:53:34.618348Z",
     "iopub.status.busy": "2026-02-05T10:53:34.617969Z",
     "iopub.status.idle": "2026-02-05T10:53:39.846574Z",
     "shell.execute_reply": "2026-02-05T10:53:39.845526Z",
     "shell.execute_reply.started": "2026-02-05T10:53:34.618316Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "title": "[code]"
   },
   "outputs": [],
   "source": [
    "df_en[\"EAST1M\"]  = pd.to_numeric(df_en[\"x\"], errors=\"coerce\")\n",
    "df_en[\"NORTH1M\"] = pd.to_numeric(df_en[\"y\"], errors=\"coerce\")\n",
    "\n",
    "df_en = df_en.dropna(subset=[\"EAST1M\",\"NORTH1M\"]).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b5a62a",
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2026-02-05T10:53:42.915985Z",
     "iopub.status.busy": "2026-02-05T10:53:42.914982Z",
     "iopub.status.idle": "2026-02-05T10:53:54.002896Z",
     "shell.execute_reply": "2026-02-05T10:53:54.001864Z",
     "shell.execute_reply.started": "2026-02-05T10:53:42.915945Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "lines_to_next_cell": 1,
    "title": "[code]"
   },
   "outputs": [],
   "source": [
    "GRID_SIZES = [1000, 5000, 10000, 25000]  # metres\n",
    "\n",
    "for g in GRID_SIZES:\n",
    "    df_en[f\"gx_{g}\"] = (df_en[\"EAST1M\"] // g) * g\n",
    "    df_en[f\"gy_{g}\"] = (df_en[\"NORTH1M\"] // g) * g\n",
    "    df_en[f\"cell_{g}\"] = (\n",
    "        df_en[f\"gx_{g}\"].astype(\"Int64\").astype(str) + \"_\" +\n",
    "        df_en[f\"gy_{g}\"].astype(\"Int64\").astype(str)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca84bb22",
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2026-02-05T10:54:15.148560Z",
     "iopub.status.busy": "2026-02-05T10:54:15.147009Z",
     "iopub.status.idle": "2026-02-05T10:54:15.879243Z",
     "shell.execute_reply": "2026-02-05T10:54:15.878036Z",
     "shell.execute_reply.started": "2026-02-05T10:54:15.148512Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "title": "[code]"
   },
   "outputs": [],
   "source": [
    "def make_cells(df, g):\n",
    "    cells = (df[[f\"gx_{g}\", f\"gy_{g}\"]]\n",
    "             .drop_duplicates()\n",
    "             .rename(columns={f\"gx_{g}\":\"gx\", f\"gy_{g}\":\"gy\"}))\n",
    "    cells[\"grid_m\"] = g\n",
    "    cells[\"x0\"] = cells[\"gx\"]\n",
    "    cells[\"y0\"] = cells[\"gy\"]\n",
    "    cells[\"x1\"] = cells[\"gx\"] + g\n",
    "    cells[\"y1\"] = cells[\"gy\"] + g\n",
    "    return cells\n",
    "\n",
    "cells_1km  = make_cells(df_en, 1000)\n",
    "cells_5km  = make_cells(df_en, 5000)\n",
    "cells_10km = make_cells(df_en, 10000)\n",
    "cells_25km = make_cells(df_en, 25000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "732ad3d2",
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2026-02-05T10:54:20.621231Z",
     "iopub.status.busy": "2026-02-05T10:54:20.620829Z",
     "iopub.status.idle": "2026-02-05T10:54:20.626788Z",
     "shell.execute_reply": "2026-02-05T10:54:20.625764Z",
     "shell.execute_reply.started": "2026-02-05T10:54:20.621200Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "title": "[code]"
   },
   "outputs": [],
   "source": [
    "print(\"1km:\",  len(cells_1km))\n",
    "print(\"5km:\",  len(cells_5km))\n",
    "print(\"10km:\", len(cells_10km))\n",
    "print(\"25km:\", len(cells_25km))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "571f813a",
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2026-02-05T10:54:23.694900Z",
     "iopub.status.busy": "2026-02-05T10:54:23.694576Z",
     "iopub.status.idle": "2026-02-05T10:54:27.954634Z",
     "shell.execute_reply": "2026-02-05T10:54:27.953561Z",
     "shell.execute_reply.started": "2026-02-05T10:54:23.694871Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "title": "[code]"
   },
   "outputs": [],
   "source": [
    "df[\"pc_key\"] = df[\"postcode\"].astype(\"string\").str.replace(\" \", \"\", regex=False).str.upper()\n",
    "df_en[\"pc_key\"] = df_en[\"PCDS\"].astype(\"string\").str.replace(\" \", \"\", regex=False).str.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "769c02e2",
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2026-02-05T06:18:15.431501Z",
     "iopub.status.busy": "2026-02-05T06:18:15.431091Z",
     "iopub.status.idle": "2026-02-05T06:18:15.666443Z",
     "shell.execute_reply": "2026-02-05T06:18:15.664087Z",
     "shell.execute_reply.started": "2026-02-05T06:18:15.431466Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "title": "[code]"
   },
   "outputs": [],
   "source": [
    "df_en.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e2b999e",
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2026-02-05T10:54:32.892609Z",
     "iopub.status.busy": "2026-02-05T10:54:32.892196Z",
     "iopub.status.idle": "2026-02-05T10:54:45.637849Z",
     "shell.execute_reply": "2026-02-05T10:54:45.636488Z",
     "shell.execute_reply.started": "2026-02-05T10:54:32.892571Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "title": "[code]"
   },
   "outputs": [],
   "source": [
    "df_en[\"EAST1M\"]  = pd.to_numeric(df_en[\"EAST1M\"], errors=\"coerce\")\n",
    "df_en[\"NORTH1M\"] = pd.to_numeric(df_en[\"NORTH1M\"], errors=\"coerce\")\n",
    "df_en = df_en.dropna(subset=[\"EAST1M\",\"NORTH1M\"]).copy()\n",
    "\n",
    "for g in [1000, 5000, 10000, 25000]:\n",
    "    df_en[f\"gx_{g}\"] = ((df_en[\"EAST1M\"] // g) * g).astype(\"int64\")\n",
    "    df_en[f\"gy_{g}\"] = ((df_en[\"NORTH1M\"] // g) * g).astype(\"int64\")\n",
    "    df_en[f\"cell_{g}\"] = df_en[f\"gx_{g}\"].astype(str) + \"_\" + df_en[f\"gy_{g}\"].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7162955",
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2026-02-05T10:54:52.022262Z",
     "iopub.status.busy": "2026-02-05T10:54:52.021894Z",
     "iopub.status.idle": "2026-02-05T10:55:04.310953Z",
     "shell.execute_reply": "2026-02-05T10:55:04.309943Z",
     "shell.execute_reply.started": "2026-02-05T10:54:52.022232Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "title": "[code]"
   },
   "outputs": [],
   "source": [
    "lookup_cols = [\n",
    "    \"pc_key\",\n",
    "    \"EAST1M\", \"NORTH1M\",\n",
    "    \"gx_1000\", \"gy_1000\", \"cell_1000\",\n",
    "    \"gx_5000\", \"gy_5000\", \"cell_5000\",\n",
    "    \"gx_10000\", \"gy_10000\", \"cell_10000\",\n",
    "    \"gx_25000\", \"gy_25000\", \"cell_25000\",\n",
    "]\n",
    "\n",
    "lookup = df_en[lookup_cols].drop_duplicates(\"pc_key\")\n",
    "\n",
    "to_drop = [c for c in lookup_cols if c in df.columns and c != \"pc_key\"]\n",
    "df = df.drop(columns=to_drop)\n",
    "\n",
    "df = df.merge(lookup, on=\"pc_key\", how=\"left\")\n",
    "\n",
    "# --- Postcode area (outcode) -> grid cell lookup tables (for UI drilldown) ---\n",
    "# Keep only the outcode (part before the space) to reduce size.\n",
    "postcode_lookup = df_en[[\n",
    "    \"PCDS\",\n",
    "    \"cell_1000\", \"cell_5000\", \"cell_10000\", \"cell_25000\",\n",
    "]].copy()\n",
    "postcode_lookup[\"outcode\"] = (\n",
    "    postcode_lookup[\"PCDS\"].astype(\"string\").str.strip().str.split(\" \", n=1).str[0].str.upper()\n",
    ")\n",
    "postcode_lookup = postcode_lookup.dropna(subset=[\"outcode\"]).drop_duplicates(\n",
    "    [\"outcode\", \"cell_1000\", \"cell_5000\", \"cell_10000\", \"cell_25000\"]\n",
    ")\n",
    "\n",
    "# Save as parquet (compact, fast) and JSON (portable)\n",
    "postcode_lookup_out_parquet = \"/kaggle/working/postcode_grid_outcode_lookup.parquet\"\n",
    "postcode_lookup_out_json = \"/kaggle/working/postcode_grid_outcode_lookup.json.gz\"\n",
    "\n",
    "postcode_lookup.to_parquet(postcode_lookup_out_parquet, index=False)\n",
    "\n",
    "import json, gzip\n",
    "with gzip.open(postcode_lookup_out_json, \"wt\", encoding=\"utf-8\") as f:\n",
    "    json.dump(postcode_lookup.where(pd.notnull(postcode_lookup), None).to_dict(orient=\"records\"), f)\n",
    "\n",
    "print(\"Wrote postcode lookup:\", postcode_lookup_out_parquet, \"rows:\", len(postcode_lookup))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25cb0668",
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2026-02-05T10:55:07.732310Z",
     "iopub.status.busy": "2026-02-05T10:55:07.731960Z",
     "iopub.status.idle": "2026-02-05T10:55:08.037267Z",
     "shell.execute_reply": "2026-02-05T10:55:08.036215Z",
     "shell.execute_reply.started": "2026-02-05T10:55:07.732280Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "title": "[code]"
   },
   "outputs": [],
   "source": [
    "df[[\"postcode\", \"EAST1M\", \"NORTH1M\", \"gx_25000\", \"gy_25000\", \"cell_25000\"]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "663e2834",
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2026-02-05T10:55:11.496227Z",
     "iopub.status.busy": "2026-02-05T10:55:11.495841Z",
     "iopub.status.idle": "2026-02-05T10:55:12.844804Z",
     "shell.execute_reply": "2026-02-05T10:55:12.843521Z",
     "shell.execute_reply.started": "2026-02-05T10:55:11.496196Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "title": "[code]"
   },
   "outputs": [],
   "source": [
    "df[\"month\"] = df[\"date\"].dt.to_period(\"M\").dt.to_timestamp()\n",
    "\n",
    "# optional but recommended for later\n",
    "df[\"property_type\"] = df[\"property_type\"].astype(\"string\")\n",
    "df[\"new_build\"] = df[\"new_build\"].astype(\"string\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e56cbf31",
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2026-02-05T10:55:15.603394Z",
     "iopub.status.busy": "2026-02-05T10:55:15.603005Z",
     "iopub.status.idle": "2026-02-05T10:55:20.349462Z",
     "shell.execute_reply": "2026-02-05T10:55:20.348451Z",
     "shell.execute_reply.started": "2026-02-05T10:55:15.603364Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "title": "[code]"
   },
   "outputs": [],
   "source": [
    "df[\"month\"] = pd.to_datetime(df[\"month\"]).dt.to_period(\"M\").dt.to_timestamp()\n",
    "latest_month = df[\"month\"].max()\n",
    "# Keep only last 10 years (inclusive, aligned to month)\n",
    "cutoff_month = (latest_month - pd.DateOffset(years=10)).to_period(\"M\").to_timestamp()\n",
    "df = df[df[\"month\"] >= cutoff_month].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc79c11",
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2026-02-05T10:55:26.523802Z",
     "iopub.status.busy": "2026-02-05T10:55:26.523449Z",
     "iopub.status.idle": "2026-02-05T10:55:26.538891Z",
     "shell.execute_reply": "2026-02-05T10:55:26.537874Z",
     "shell.execute_reply.started": "2026-02-05T10:55:26.523771Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "lines_to_next_cell": 1,
    "title": "[code]"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def _yearly_end_months(d_month_col: pd.Series, years_back: int):\n",
    "    latest = d_month_col.max()\n",
    "    end_months = [latest]\n",
    "    cursor = latest - pd.DateOffset(years=1)\n",
    "    min_month = d_month_col.min()\n",
    "    while cursor >= min_month and len(end_months) < (years_back + 1):\n",
    "        end_months.append(cursor)\n",
    "        cursor -= pd.DateOffset(years=1)\n",
    "    return end_months\n",
    "\n",
    "def make_grid_annual_stack_levels(df, g, min_sales=3, years_back=10):\n",
    "    \"\"\"\n",
    "    Returns ONE dataframe containing 4 levels:\n",
    "      - TYPE+BUILD\n",
    "      - TYPE+ALL\n",
    "      - ALL+BUILD\n",
    "      - ALL+ALL\n",
    "    All use true medians (transaction-level) over trailing 12 months.\n",
    "    \"\"\"\n",
    "    gx, gy = f\"gx_{g}\", f\"gy_{g}\"\n",
    "\n",
    "    d = df.dropna(subset=[gx, gy]).copy()\n",
    "    d[\"month\"] = pd.to_datetime(d[\"month\"]).dt.to_period(\"M\").dt.to_timestamp()\n",
    "\n",
    "    end_months = _yearly_end_months(d[\"month\"], years_back=years_back)\n",
    "\n",
    "    parts = []\n",
    "    for end_month in end_months:\n",
    "        start_month = (end_month - pd.DateOffset(months=11)).to_period(\"M\").to_timestamp()\n",
    "        w = d[(d[\"month\"] >= start_month) & (d[\"month\"] <= end_month)]\n",
    "\n",
    "        # 1) TYPE + BUILD\n",
    "        a = (w.groupby([gx, gy, \"property_type\", \"new_build\"], as_index=False)\n",
    "               .agg(median_price_12m=(\"price\", \"median\"),\n",
    "                    sales_12m=(\"price\", \"size\")))\n",
    "        a[\"end_month\"] = end_month\n",
    "        parts.append(a)\n",
    "\n",
    "        # 2) TYPE + ALL_BUILD\n",
    "        b = (w.groupby([gx, gy, \"property_type\"], as_index=False)\n",
    "               .agg(median_price_12m=(\"price\", \"median\"),\n",
    "                    sales_12m=(\"price\", \"size\")))\n",
    "        b[\"end_month\"] = end_month\n",
    "        b[\"new_build\"] = \"ALL\"\n",
    "        parts.append(b)\n",
    "\n",
    "        # 3) ALL_TYPE + BUILD\n",
    "        c = (w.groupby([gx, gy, \"new_build\"], as_index=False)\n",
    "               .agg(median_price_12m=(\"price\", \"median\"),\n",
    "                    sales_12m=(\"price\", \"size\")))\n",
    "        c[\"end_month\"] = end_month\n",
    "        c[\"property_type\"] = \"ALL\"\n",
    "        parts.append(c)\n",
    "\n",
    "        # 4) ALL_TYPE + ALL_BUILD\n",
    "        d_all = (w.groupby([gx, gy], as_index=False)\n",
    "                   .agg(median_price_12m=(\"price\", \"median\"),\n",
    "                        sales_12m=(\"price\", \"size\")))\n",
    "        d_all[\"end_month\"] = end_month\n",
    "        d_all[\"property_type\"] = \"ALL\"\n",
    "        d_all[\"new_build\"] = \"ALL\"\n",
    "        parts.append(d_all)\n",
    "\n",
    "        print(\"done:\", end_month.date(), \"| window rows:\", len(w))\n",
    "\n",
    "    out = pd.concat(parts, ignore_index=True)\n",
    "\n",
    "    # tidy\n",
    "    out[\"end_month\"] = pd.to_datetime(out[\"end_month\"]).dt.to_period(\"M\").dt.to_timestamp()\n",
    "    out[\"property_type\"] = out[\"property_type\"].astype(str)\n",
    "    out[\"new_build\"] = out[\"new_build\"].astype(str)\n",
    "\n",
    "    if min_sales > 1:\n",
    "        out = out[out[\"sales_12m\"] >= min_sales].copy()\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cce73c5",
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2026-02-05T10:55:32.777811Z",
     "iopub.status.busy": "2026-02-05T10:55:32.777391Z",
     "iopub.status.idle": "2026-02-05T10:55:44.224520Z",
     "shell.execute_reply": "2026-02-05T10:55:44.223589Z",
     "shell.execute_reply.started": "2026-02-05T10:55:32.777777Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "title": "[code]"
   },
   "outputs": [],
   "source": [
    "grid_25km_annual = make_grid_annual_stack_levels(\n",
    "    df,\n",
    "    g=25000,\n",
    "    min_sales=3,\n",
    "    years_back=10   # latest + last 10 yearly snapshots\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fdba2a0",
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2026-02-05T10:55:50.350187Z",
     "iopub.status.busy": "2026-02-05T10:55:50.349854Z",
     "iopub.status.idle": "2026-02-05T10:56:01.955021Z",
     "shell.execute_reply": "2026-02-05T10:56:01.954066Z",
     "shell.execute_reply.started": "2026-02-05T10:55:50.350161Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "title": "[code]"
   },
   "outputs": [],
   "source": [
    "grid_10km_annual = make_grid_annual_stack_levels(\n",
    "    df,\n",
    "    g=10000,\n",
    "    min_sales=3,\n",
    "    years_back=10   # latest + last 10 yearly snapshots\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b2dba4",
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2026-02-05T10:56:05.925903Z",
     "iopub.status.busy": "2026-02-05T10:56:05.925574Z",
     "iopub.status.idle": "2026-02-05T10:56:21.921453Z",
     "shell.execute_reply": "2026-02-05T10:56:21.919843Z",
     "shell.execute_reply.started": "2026-02-05T10:56:05.925875Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "title": "[code]"
   },
   "outputs": [],
   "source": [
    "grid_5km_annual = make_grid_annual_stack_levels(\n",
    "    df,\n",
    "    g=5000,\n",
    "    min_sales=3,\n",
    "    years_back=10   # latest + last 10 yearly snapshots\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea38a9f1",
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2026-02-05T11:38:31.189664Z",
     "iopub.status.busy": "2026-02-05T11:38:31.184865Z",
     "iopub.status.idle": "2026-02-05T11:38:44.470826Z",
     "shell.execute_reply": "2026-02-05T11:38:44.469449Z",
     "shell.execute_reply.started": "2026-02-05T11:38:31.189576Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "title": "[code]"
   },
   "outputs": [],
   "source": [
    "grid_1km_annual = make_grid_annual_stack_levels(\n",
    "    df,\n",
    "    g=1000,\n",
    "    min_sales=3,\n",
    "    years_back=1   # latest + last 10 yearly snapshots\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1638918d",
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2026-02-05T11:38:52.539963Z",
     "iopub.status.busy": "2026-02-05T11:38:52.539283Z",
     "iopub.status.idle": "2026-02-05T11:38:52.730436Z",
     "shell.execute_reply": "2026-02-05T11:38:52.729134Z",
     "shell.execute_reply.started": "2026-02-05T11:38:52.539929Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "title": "[code]"
   },
   "outputs": [],
   "source": [
    "grid_1km_annual.to_parquet(\"/kaggle/working/grid_1km_annual.parquet\", index=False)\n",
    "grid_5km_annual.to_parquet(\"/kaggle/working/grid_5km_annual.parquet\", index=False)\n",
    "grid_10km_annual.to_parquet(\"/kaggle/working/grid_10km_annual.parquet\", index=False)\n",
    "grid_25km_annual.to_parquet(\"/kaggle/working/grid_25km_annual.parquet\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6978e637",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-05T10:58:33.489774Z",
     "iopub.status.busy": "2026-02-05T10:58:33.489386Z",
     "iopub.status.idle": "2026-02-05T10:59:06.948154Z",
     "shell.execute_reply": "2026-02-05T10:59:06.946920Z",
     "shell.execute_reply.started": "2026-02-05T10:58:33.489744Z"
    },
    "lines_to_next_cell": 2,
    "title": "[code]"
   },
   "outputs": [],
   "source": [
    "import pandas as pd, json, gzip\n",
    "\n",
    "d = grid_1km_annual.copy()\n",
    "\n",
    "g = 1000\n",
    "d = d.rename(columns={\n",
    "    f\"gx_{g}\": \"gx\",\n",
    "    f\"gy_{g}\": \"gy\",\n",
    "    \"median_price_12m\": \"median\",\n",
    "    \"sales_12m\": \"tx_count\",\n",
    "})\n",
    "\n",
    "# Normalize end_month to ISO format (YYYY-MM-DD) required for API filtering\n",
    "d[\"end_month\"] = pd.to_datetime(d[\"end_month\"]).dt.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "keep = [\"gx\",\"gy\",\"end_month\",\"property_type\",\"new_build\",\"median\",\"tx_count\"]\n",
    "d = d[keep].dropna(subset=[\"gx\",\"gy\",\"median\"]).copy()\n",
    "\n",
    "out_path = \"/kaggle/working/grid_1km_full.json.gz\"\n",
    "with gzip.open(out_path, \"wt\", encoding=\"utf-8\") as f:\n",
    "    # JSON array (easy to parse/cached in worker)\n",
    "    json.dump(d.to_dict(orient=\"records\"), f)\n",
    "\n",
    "print(\"Wrote:\", out_path, \"rows:\", len(d))\n",
    "\n",
    "# Extract and store metadata: available date ranges for each dataset\n",
    "grid_metadata = {}\n",
    "for grid_size, grid_annual in [\n",
    "    (1000, grid_1km_annual),\n",
    "    (5000, grid_5km_annual),\n",
    "    (10000, grid_10km_annual),\n",
    "    (25000, grid_25km_annual),\n",
    "]:\n",
    "    grid_label = f\"{grid_size // 1000}km\"\n",
    "    earliest = pd.to_datetime(grid_annual[\"end_month\"].min()).strftime(\"%Y-%m-%d\")\n",
    "    latest = pd.to_datetime(grid_annual[\"end_month\"].max()).strftime(\"%Y-%m-%d\")\n",
    "    grid_metadata[grid_label] = {\n",
    "        \"earliest\": earliest,\n",
    "        \"latest\": latest,\n",
    "        \"available_months\": int(grid_annual[\"end_month\"].nunique())\n",
    "    }\n",
    "\n",
    "# Save metadata JSON\n",
    "metadata_path = \"/kaggle/working/grid_metadata.json\"\n",
    "with open(metadata_path, \"w\") as f:\n",
    "    json.dump(grid_metadata, f, indent=2)\n",
    "\n",
    "print(f\"\\nGrid availability metadata:\")\n",
    "print(json.dumps(grid_metadata, indent=2))\n",
    "print(f\"Saved to {metadata_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e23845ba",
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2026-02-05T10:59:19.568536Z",
     "iopub.status.busy": "2026-02-05T10:59:19.568087Z",
     "iopub.status.idle": "2026-02-05T10:59:22.362539Z",
     "shell.execute_reply": "2026-02-05T10:59:22.361516Z",
     "shell.execute_reply.started": "2026-02-05T10:59:19.568502Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "title": "[code]"
   },
   "outputs": [],
   "source": [
    "g = 25000\n",
    "\n",
    "start_month = pd.Timestamp(\"2025-01-01\")\n",
    "end_month   = pd.Timestamp(\"2025-12-01\")\n",
    "\n",
    "count = df[\n",
    "    (df[f\"gx_{g}\"] == 500000) &\n",
    "    (df[f\"gy_{g}\"] == 200000) &\n",
    "    (df[\"property_type\"] == \"D\") &\n",
    "    (df[\"new_build\"] == \"Y\") &\n",
    "    (pd.to_datetime(df[\"month\"]).dt.to_period(\"M\").dt.to_timestamp() >= start_month) &\n",
    "    (pd.to_datetime(df[\"month\"]).dt.to_period(\"M\").dt.to_timestamp() <= end_month)\n",
    "].shape[0]\n",
    "\n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59760db2",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "title": "[code]"
   },
   "outputs": [],
   "source": [
    "(grid_25km_annual[\"property_type\"].eq(\"ALL\") & grid_25km_annual[\"new_build\"].eq(\"ALL\")).any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "646d2201",
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2026-02-05T10:59:27.946456Z",
     "iopub.status.busy": "2026-02-05T10:59:27.946071Z",
     "iopub.status.idle": "2026-02-05T10:59:27.968472Z",
     "shell.execute_reply": "2026-02-05T10:59:27.967260Z",
     "shell.execute_reply.started": "2026-02-05T10:59:27.946426Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "title": "[code]"
   },
   "outputs": [],
   "source": [
    "g = 25000\n",
    "\n",
    "row = grid_25km_annual[\n",
    "    (grid_25km_annual[f\"gx_{g}\"] == 500000) &\n",
    "    (grid_25km_annual[f\"gy_{g}\"] == 200000) &\n",
    "    (grid_25km_annual[\"property_type\"] == \"D\") &\n",
    "    (grid_25km_annual[\"new_build\"] == \"ALL\") \n",
    "]\n",
    "\n",
    "row.sort_values(by='sales_12m' , ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb3c06d",
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2026-02-05T10:59:32.912331Z",
     "iopub.status.busy": "2026-02-05T10:59:32.911333Z",
     "iopub.status.idle": "2026-02-05T10:59:37.368910Z",
     "shell.execute_reply": "2026-02-05T10:59:37.367632Z",
     "shell.execute_reply.started": "2026-02-05T10:59:32.912295Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "lines_to_next_cell": 1,
    "title": "[code]"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyproj import Transformer\n",
    "import plotly.express as px\n",
    "import plotly.io as pio\n",
    "\n",
    "pio.renderers.default = \"iframe\"\n",
    "TRANSFORMER = Transformer.from_crs(\"EPSG:27700\", \"EPSG:4326\", always_xy=True)\n",
    "\n",
    "colorscale = \"RdYlGn_r\"\n",
    "lo = 200_000\n",
    "hi = 1_000_000\n",
    "\n",
    "def plot_annual_grid_map(\n",
    "    grid_annual: pd.DataFrame,\n",
    "    g: int,\n",
    "    end_month=None,\n",
    "    property_type: str = \"ALL\",\n",
    "    new_build: str = \"ALL\",\n",
    "    min_sales: int = 3,\n",
    "    use_tiles: bool = True,\n",
    "    zoom: float = 4.6,\n",
    "    opacity: float = 0.45,\n",
    "    fill_gaps: bool = True,\n",
    "    lookback_years: int = 10\n",
    "):\n",
    "    def norm_month_series(s: pd.Series) -> pd.Series:\n",
    "        return pd.to_datetime(s.astype(str)).dt.to_period(\"M\").dt.to_timestamp()\n",
    "\n",
    "    def norm_month_value(v) -> pd.Timestamp:\n",
    "        return pd.to_datetime(str(v)).to_period(\"M\").to_timestamp()\n",
    "\n",
    "    gx_col = f\"gx_{g}\"\n",
    "    gy_col = f\"gy_{g}\"\n",
    "    cell_col = f\"cell_{g}\"\n",
    "\n",
    "    d = grid_annual.copy()\n",
    "    d[\"end_month\"] = norm_month_series(d[\"end_month\"])\n",
    "\n",
    "    if end_month is None:\n",
    "        end_month = d[\"end_month\"].max()\n",
    "    else:\n",
    "        end_month = norm_month_value(end_month)\n",
    "\n",
    "    # Filter to segment first (required)\n",
    "    d = d[(d[\"property_type\"] == property_type) & (d[\"new_build\"] == new_build)].copy()\n",
    "    if d.empty:\n",
    "        raise ValueError(\"No rows after filtering property_type/new_build. Check your stacked levels exist.\")\n",
    "\n",
    "    # Standardize columns\n",
    "    d = d.rename(columns={\"median_price_12m\": \"median_price\", \"sales_12m\": \"sales\"})\n",
    "\n",
    "    # Restrict to eligible backfill window\n",
    "    min_allowed = norm_month_value(end_month - pd.DateOffset(years=lookback_years))\n",
    "    d = d[(d[\"end_month\"] <= end_month) & (d[\"end_month\"] >= min_allowed)].copy()\n",
    "\n",
    "    if fill_gaps:\n",
    "        d = (\n",
    "            d.sort_values(\"end_month\", ascending=False)\n",
    "             .drop_duplicates(subset=[gx_col, gy_col], keep=\"first\")\n",
    "             .copy()\n",
    "        )\n",
    "        d[\"end_month_used\"] = d[\"end_month\"]\n",
    "    else:\n",
    "        d = d[d[\"end_month\"] == end_month].copy()\n",
    "        d[\"end_month_used\"] = d[\"end_month\"]\n",
    "\n",
    "    if d.empty:\n",
    "        raise ValueError(\"No rows left after applying end_month / backfill window.\")\n",
    "\n",
    "    # Sales threshold\n",
    "    d = d[d[\"sales\"] >= min_sales].copy()\n",
    "    if d.empty:\n",
    "        raise ValueError(\"No rows meet min_sales after filtering/backfill.\")\n",
    "\n",
    "    d[\"years_stale\"] = (end_month.year - d[\"end_month_used\"].map(lambda t: t.year)).astype(int)\n",
    "\n",
    "    # Cell id\n",
    "    d[cell_col] = d[gx_col].astype(\"int64\").astype(str) + \"_\" + d[gy_col].astype(\"int64\").astype(str)\n",
    "\n",
    "    # Build GeoJSON squares\n",
    "    x0 = d[gx_col].astype(float).to_numpy()\n",
    "    y0 = d[gy_col].astype(float).to_numpy()\n",
    "    x1 = x0 + g\n",
    "    y1 = y0 + g\n",
    "\n",
    "    lon00, lat00 = TRANSFORMER.transform(x0, y0)\n",
    "    lon10, lat10 = TRANSFORMER.transform(x1, y0)\n",
    "    lon11, lat11 = TRANSFORMER.transform(x1, y1)\n",
    "    lon01, lat01 = TRANSFORMER.transform(x0, y1)\n",
    "\n",
    "    lon_min, lon_max = -10.5, 4.5\n",
    "    lat_min, lat_max = 49.0, 62.5\n",
    "    ok = (\n",
    "        (lon00 >= lon_min) & (lon00 <= lon_max) & (lat00 >= lat_min) & (lat00 <= lat_max) &\n",
    "        (lon10 >= lon_min) & (lon10 <= lon_max) & (lat10 >= lat_min) & (lat10 <= lat_max) &\n",
    "        (lon11 >= lon_min) & (lon11 <= lon_max) & (lat11 >= lat_min) & (lat11 <= lat_max) &\n",
    "        (lon01 >= lon_min) & (lon01 <= lon_max) & (lat01 >= lat_min) & (lat01 <= lat_max)\n",
    "    )\n",
    "\n",
    "    d = d.loc[ok].copy()\n",
    "    if d.empty:\n",
    "        raise ValueError(\"Nothing left after UK clipping (check gx/gy values).\")\n",
    "\n",
    "    idx = np.where(ok)[0]\n",
    "    lon00, lat00 = lon00[idx], lat00[idx]\n",
    "    lon10, lat10 = lon10[idx], lat10[idx]\n",
    "    lon11, lat11 = lon11[idx], lat11[idx]\n",
    "    lon01, lat01 = lon01[idx], lat01[idx]\n",
    "\n",
    "    ids = d[cell_col].astype(str).to_numpy()\n",
    "\n",
    "    features = []\n",
    "    for i in range(len(d)):\n",
    "        poly = [\n",
    "            [lon00[i], lat00[i]],\n",
    "            [lon10[i], lat10[i]],\n",
    "            [lon11[i], lat11[i]],\n",
    "            [lon01[i], lat01[i]],\n",
    "            [lon00[i], lat00[i]],\n",
    "        ]\n",
    "        features.append({\n",
    "            \"type\": \"Feature\",\n",
    "            \"id\": ids[i],\n",
    "            \"properties\": {\"cell\": ids[i]},\n",
    "            \"geometry\": {\"type\": \"Polygon\", \"coordinates\": [poly]},\n",
    "        })\n",
    "\n",
    "    geojson = {\"type\": \"FeatureCollection\", \"features\": features}\n",
    "\n",
    "    lo = float(d[\"median_price\"].quantile(0.05))\n",
    "    hi = float(d[\"median_price\"].quantile(0.95))\n",
    "\n",
    "    title = f\"UK House Prices ({g//1000}km) — target {end_month.date()} — {property_type}/{new_build}\"\n",
    "    if fill_gaps:\n",
    "        title += f\" (backfilled ≤{lookback_years}y)\"\n",
    "\n",
    "    hover = {\n",
    "        \"median_price\": \":,.0f\",\n",
    "        \"sales\": True,\n",
    "        \"end_month_used\": True,\n",
    "        \"years_stale\": True,\n",
    "        \"property_type\": True,\n",
    "        \"new_build\": True,\n",
    "    }\n",
    "\n",
    "    if use_tiles:\n",
    "        fig = px.choropleth_mapbox(\n",
    "            d,\n",
    "            geojson=geojson,\n",
    "            locations=cell_col,\n",
    "            color=\"median_price\",\n",
    "            range_color=(lo, hi),\n",
    "            color_continuous_scale=colorscale,\n",
    "            hover_data=hover,\n",
    "            mapbox_style=\"open-street-map\",\n",
    "            center={\"lat\": 54.5, \"lon\": -2.5},\n",
    "            zoom=zoom,\n",
    "            opacity=opacity,\n",
    "            title=title\n",
    "        )\n",
    "    else:\n",
    "        fig = px.choropleth(\n",
    "            d,\n",
    "            geojson=geojson,\n",
    "            locations=cell_col,\n",
    "            color=\"median_price\",\n",
    "            range_color=(lo, hi),\n",
    "            hover_data=hover,\n",
    "            title=title\n",
    "        )\n",
    "        fig.update_geos(fitbounds=\"locations\", visible=False)\n",
    "\n",
    "    fig.update_traces(marker_line_width=0.3)\n",
    "    fig.update_layout(margin={\"r\": 0, \"t\": 55, \"l\": 0, \"b\": 0})\n",
    "    fig.update_traces(marker_line_width=1.4, marker_line_color=\"rgba(0,0,0,0.7)\")\n",
    "    fig.show(renderer=\"iframe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e5d562",
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2026-02-05T10:59:49.898978Z",
     "iopub.status.busy": "2026-02-05T10:59:49.898611Z",
     "iopub.status.idle": "2026-02-05T10:59:52.446480Z",
     "shell.execute_reply": "2026-02-05T10:59:52.445385Z",
     "shell.execute_reply.started": "2026-02-05T10:59:49.898950Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "title": "[code]"
   },
   "outputs": [],
   "source": [
    "g = 25000\n",
    "grid_annual = grid_25km_annual  # or grid_10km_annual / grid_5km_annual / grid_1km_annual\n",
    "\n",
    "d_map = plot_annual_grid_map(\n",
    "    grid_annual,\n",
    "    g=g,\n",
    "    end_month=\"2025-12-01\",\n",
    "    property_type=\"D\",\n",
    "    new_build=\"N\",\n",
    "    opacity=0.35,\n",
    "    fill_gaps=True,\n",
    "    lookback_years=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c333188",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "lines_to_next_cell": 1,
    "title": "[code]"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "\n",
    "def build_delta_df(grid_annual: pd.DataFrame, g: int,\n",
    "                   property_type: str = \"ALL\", new_build: str = \"ALL\",\n",
    "                   min_sales: int = 3,\n",
    "                   end_month_latest=None,\n",
    "                   end_month_oldest=None) -> pd.DataFrame:\n",
    "    gx_col = f\"gx_{g}\"\n",
    "    gy_col = f\"gy_{g}\"\n",
    "    cell_col = f\"cell_{g}\"\n",
    "\n",
    "    d = grid_annual.copy()\n",
    "    d[\"end_month\"] = pd.to_datetime(d[\"end_month\"].astype(str)).dt.to_period(\"M\").dt.to_timestamp()\n",
    "\n",
    "    # segment filter (must be explicit to avoid dupes)\n",
    "    d = d[(d[\"property_type\"] == property_type) & (d[\"new_build\"] == new_build)].copy()\n",
    "    if d.empty:\n",
    "        raise ValueError(\"No rows for that property_type/new_build. (Do you have ALL/ALL stacked?)\")\n",
    "\n",
    "    d = d.rename(columns={\"median_price_12m\": \"median_price\", \"sales_12m\": \"sales\"})\n",
    "    d = d[d[\"sales\"] >= min_sales].copy()\n",
    "\n",
    "    # choose snapshot months\n",
    "    if end_month_latest is None:\n",
    "        end_month_latest = d[\"end_month\"].max()\n",
    "    else:\n",
    "        end_month_latest = pd.to_datetime(str(end_month_latest)).to_period(\"M\").to_timestamp()\n",
    "\n",
    "    if end_month_oldest is None:\n",
    "        end_month_oldest = d[\"end_month\"].min()\n",
    "    else:\n",
    "        end_month_oldest = pd.to_datetime(str(end_month_oldest)).to_period(\"M\").to_timestamp()\n",
    "\n",
    "    latest = d[d[\"end_month\"] == end_month_latest].copy()\n",
    "    oldest = d[d[\"end_month\"] == end_month_oldest].copy()\n",
    "\n",
    "    if latest.empty or oldest.empty:\n",
    "        raise ValueError(\"Chosen end_month_latest/end_month_oldest not found for that segment.\")\n",
    "\n",
    "    keep = [gx_col, gy_col, \"median_price\", \"sales\"]\n",
    "    latest = latest[keep].rename(columns={\"median_price\": \"price_latest\", \"sales\": \"sales_latest\"})\n",
    "    oldest = oldest[keep].rename(columns={\"median_price\": \"price_oldest\", \"sales\": \"sales_oldest\"})\n",
    "\n",
    "    out = latest.merge(oldest, on=[gx_col, gy_col], how=\"inner\")\n",
    "\n",
    "    out[\"delta_gbp\"] = out[\"price_latest\"] - out[\"price_oldest\"]\n",
    "    out[\"delta_pct\"] = np.where(out[\"price_oldest\"] > 0,\n",
    "                                (out[\"price_latest\"] / out[\"price_oldest\"] - 1.0) * 100.0,\n",
    "                                np.nan)\n",
    "\n",
    "    out[cell_col] = out[gx_col].astype(\"int64\").astype(str) + \"_\" + out[gy_col].astype(\"int64\").astype(str)\n",
    "    out[\"end_month_latest\"] = end_month_latest\n",
    "    out[\"end_month_oldest\"] = end_month_oldest\n",
    "    out[\"property_type\"] = property_type\n",
    "    out[\"new_build\"] = new_build\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0884ec4",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "lines_to_next_cell": 1,
    "title": "[code]"
   },
   "outputs": [],
   "source": [
    "from pyproj import Transformer\n",
    "\n",
    "TRANSFORMER = Transformer.from_crs(\"EPSG:27700\", \"EPSG:4326\", always_xy=True)\n",
    "\n",
    "def plot_delta_map(delta_df: pd.DataFrame, g: int,\n",
    "                   metric: str = \"delta_pct\",  # \"delta_pct\" or \"delta_gbp\"\n",
    "                   use_tiles: bool = True,\n",
    "                   zoom: float = 4.6,\n",
    "                   opacity: float = 0.45):\n",
    "    gx_col = f\"gx_{g}\"\n",
    "    gy_col = f\"gy_{g}\"\n",
    "    cell_col = f\"cell_{g}\"\n",
    "\n",
    "    d = delta_df.copy()\n",
    "\n",
    "    # GeoJSON squares\n",
    "    x0 = d[gx_col].astype(float).to_numpy()\n",
    "    y0 = d[gy_col].astype(float).to_numpy()\n",
    "    x1 = x0 + g\n",
    "    y1 = y0 + g\n",
    "\n",
    "    lon00, lat00 = TRANSFORMER.transform(x0, y0)\n",
    "    lon10, lat10 = TRANSFORMER.transform(x1, y0)\n",
    "    lon11, lat11 = TRANSFORMER.transform(x1, y1)\n",
    "    lon01, lat01 = TRANSFORMER.transform(x0, y1)\n",
    "\n",
    "    # UK-ish clip\n",
    "    ok = (\n",
    "        (lon00 >= -10.5) & (lon00 <= 4.5) & (lat00 >= 49.0) & (lat00 <= 62.5) &\n",
    "        (lon10 >= -10.5) & (lon10 <= 4.5) & (lat10 >= 49.0) & (lat10 <= 62.5) &\n",
    "        (lon11 >= -10.5) & (lon11 <= 4.5) & (lat11 >= 49.0) & (lat11 <= 62.5) &\n",
    "        (lon01 >= -10.5) & (lon01 <= 4.5) & (lat01 >= 49.0) & (lat01 <= 62.5)\n",
    "    )\n",
    "    d = d.loc[ok].copy()\n",
    "    idx = np.where(ok)[0]\n",
    "    lon00, lat00 = lon00[idx], lat00[idx]\n",
    "    lon10, lat10 = lon10[idx], lat10[idx]\n",
    "    lon11, lat11 = lon11[idx], lat11[idx]\n",
    "    lon01, lat01 = lon01[idx], lat01[idx]\n",
    "\n",
    "    ids = d[cell_col].astype(str).to_numpy()\n",
    "    features = []\n",
    "    for i in range(len(d)):\n",
    "        poly = [[lon00[i], lat00[i]],[lon10[i], lat10[i]],[lon11[i], lat11[i]],[lon01[i], lat01[i]],[lon00[i], lat00[i]]]\n",
    "        features.append({\"type\":\"Feature\",\"id\":ids[i],\"properties\":{\"cell\":ids[i]},\n",
    "                         \"geometry\":{\"type\":\"Polygon\",\"coordinates\":[poly]}})\n",
    "    geojson = {\"type\":\"FeatureCollection\",\"features\":features}\n",
    "\n",
    "    end_latest = pd.to_datetime(d[\"end_month_latest\"].iloc[0]).date()\n",
    "    end_oldest = pd.to_datetime(d[\"end_month_oldest\"].iloc[0]).date()\n",
    "    seg = f\"{d['property_type'].iloc[0]}/{d['new_build'].iloc[0]}\"\n",
    "\n",
    "    # Diverging colour scale for deltas\n",
    "    colorscale = \"RdBu\"  # red=negative, blue=positive by default\n",
    "    if metric == \"delta_pct\":\n",
    "        # centre at 0 using symmetric range based on 95th percentile\n",
    "        m = float(np.nanpercentile(np.abs(d[\"delta_pct\"]), 95))\n",
    "        rng = (-m, m)\n",
    "        title = f\"Δ% (12m ending {end_oldest} → {end_latest}) — {seg}\"\n",
    "        hover = {\"delta_pct\":\":.1f\",\"delta_gbp\":\":,.0f\",\"price_oldest\":\":,.0f\",\"price_latest\":\":,.0f\",\n",
    "                 \"sales_oldest\":True,\"sales_latest\":True}\n",
    "        color_col = \"delta_pct\"\n",
    "    else:\n",
    "        m = float(np.nanpercentile(np.abs(d[\"delta_gbp\"]), 95))\n",
    "        rng = (-m, m)\n",
    "        title = f\"Δ£ (12m ending {end_oldest} → {end_latest}) — {seg}\"\n",
    "        hover = {\"delta_gbp\":\":,.0f\",\"delta_pct\":\":.1f\",\"price_oldest\":\":,.0f\",\"price_latest\":\":,.0f\",\n",
    "                 \"sales_oldest\":True,\"sales_latest\":True}\n",
    "        color_col = \"delta_gbp\"\n",
    "\n",
    "    if use_tiles:\n",
    "        fig = px.choropleth_mapbox(\n",
    "            d, geojson=geojson, locations=cell_col, color=color_col,\n",
    "            range_color=rng, color_continuous_scale=colorscale,\n",
    "            hover_data=hover, mapbox_style=\"open-street-map\",\n",
    "            center={\"lat\":54.5,\"lon\":-2.5}, zoom=zoom, opacity=opacity, title=title\n",
    "        )\n",
    "    else:\n",
    "        fig = px.choropleth(\n",
    "            d, geojson=geojson, locations=cell_col, color=color_col,\n",
    "            range_color=rng, color_continuous_scale=colorscale,\n",
    "            hover_data=hover, title=title\n",
    "        )\n",
    "        fig.update_geos(fitbounds=\"locations\", visible=False)\n",
    "\n",
    "    # darker borders like you wanted\n",
    "    fig.update_traces(marker_line_width=1.1, marker_line_color=\"rgba(0,0,0,0.9)\")\n",
    "    fig.update_layout(margin={\"r\":0,\"t\":55,\"l\":0,\"b\":0})\n",
    "    fig.show(renderer=\"iframe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "443aaad5",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "lines_to_next_cell": 1,
    "title": "[code]"
   },
   "outputs": [],
   "source": [
    "def plot_top_movers(delta_df: pd.DataFrame, n: int = 20):\n",
    "    d = delta_df.dropna(subset=[\"delta_pct\"]).copy()\n",
    "    d = d.sort_values(\"delta_pct\", ascending=False).head(n).copy()\n",
    "\n",
    "    fig = px.bar(\n",
    "        d,\n",
    "        x=\"delta_pct\",\n",
    "        y=d.index.astype(str),\n",
    "        hover_data={\"delta_pct\":\":.1f\",\"delta_gbp\":\":,.0f\",\"price_oldest\":\":,.0f\",\"price_latest\":\":,.0f\"},\n",
    "        title=f\"Top {n} grid cells by Δ% (joined cells only)\"\n",
    "    )\n",
    "    fig.update_layout(yaxis_title=\"row\", xaxis_title=\"Δ%\")\n",
    "    fig.show(renderer=\"iframe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c2642dd",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "lines_to_next_cell": 1,
    "title": "[code]"
   },
   "outputs": [],
   "source": [
    "def build_overall_deltas(grid_annual: pd.DataFrame, min_sales: int = 3) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Compute deltas between earliest and latest available month across entire dataset.\n",
    "    \n",
    "    KAGGLE OPTIMIZATION:\n",
    "    - Avoids storing all intermediate period deltas; only computes earliest→latest\n",
    "    - Single merge operation (lower memory footprint than multi-period approach)\n",
    "    - Filtered by min_sales to reduce output size\n",
    "    - Returns only non-null deltas to save storage\n",
    "    \n",
    "    Args:\n",
    "        grid_annual: Annual stacked grid data with columns [gx_*, gy_*, end_month, property_type, new_build, median, sales_12m]\n",
    "        min_sales: Minimum transaction count to include\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with columns [gx, gy, cell, property_type, new_build, \n",
    "        price_earliest, sales_earliest, end_month_earliest,\n",
    "        price_latest, sales_latest, end_month_latest,\n",
    "        delta_gbp, delta_pct, years_delta]\n",
    "    \"\"\"\n",
    "    d = grid_annual.copy()\n",
    "    print(f\"DEBUG: build_overall_deltas input: {len(d)} rows, min_sales filter={min_sales}\")\n",
    "    d = d[d[\"sales_12m\"] >= min_sales].copy()\n",
    "    print(f\"DEBUG: after min_sales filter: {len(d)} rows\")\n",
    "    \n",
    "    if d.empty:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Find earliest and latest months available\n",
    "    earliest_month = d[\"end_month\"].min()\n",
    "    latest_month = d[\"end_month\"].max()\n",
    "    \n",
    "    print(f\"DEBUG: earliest_month={earliest_month}, latest_month={latest_month}\")\n",
    "    \n",
    "    if earliest_month == latest_month:\n",
    "        print(f\"Only one month available ({earliest_month}); skipping overall deltas\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Subset to earliest and latest only (memory efficient)\n",
    "    earliest_data = d[d[\"end_month\"] == earliest_month].copy()\n",
    "    latest_data = d[d[\"end_month\"] == latest_month].copy()\n",
    "    \n",
    "    # Determine grid size from column names (needed for debug output)\n",
    "    grid_sizes = [g for col in earliest_data.columns if col.startswith(\"gx_\") for g in [int(col.split(\"_\")[1])]]\n",
    "    if not grid_sizes:\n",
    "        raise ValueError(\"No grid columns found (expected gx_*, gy_*)\")\n",
    "    g = grid_sizes[0]\n",
    "    \n",
    "    gx_col = f\"gx_{g}\"\n",
    "    gy_col = f\"gy_{g}\"\n",
    "    cell_col = f\"cell_{g}\"\n",
    "    \n",
    "    print(f\"DEBUG earliest_data rows: {len(earliest_data)}, unique cells: {earliest_data[[gx_col, gy_col]].drop_duplicates().shape[0]}\")\n",
    "    print(f\"DEBUG latest_data rows: {len(latest_data)}, unique cells: {latest_data[[gx_col, gy_col]].drop_duplicates().shape[0]}\")\n",
    "    \n",
    "    # Group by cell, property_type, new_build at each time point\n",
    "    earliest_agg = earliest_data.groupby([gx_col, gy_col, \"property_type\", \"new_build\"]).agg({\n",
    "        \"median_price_12m\": \"first\",\n",
    "        \"sales_12m\": \"first\"\n",
    "    }).reset_index().rename(columns={\"median_price_12m\": \"price_earliest\", \"sales_12m\": \"sales_earliest\"})\n",
    "    \n",
    "    latest_agg = latest_data.groupby([gx_col, gy_col, \"property_type\", \"new_build\"]).agg({\n",
    "        \"median_price_12m\": \"first\",\n",
    "        \"sales_12m\": \"first\"\n",
    "    }).reset_index().rename(columns={\"median_price_12m\": \"price_latest\", \"sales_12m\": \"sales_latest\"})\n",
    "    \n",
    "    print(f\"DEBUG: earliest_agg={len(earliest_agg)} unique cell+segment combos\")\n",
    "    print(f\"DEBUG: latest_agg={len(latest_agg)} unique cell+segment combos\")\n",
    "    \n",
    "    # Inner join: only cells with data in both periods\n",
    "    out = earliest_agg.merge(\n",
    "        latest_agg,\n",
    "        on=[gx_col, gy_col, \"property_type\", \"new_build\"],\n",
    "        how=\"inner\"\n",
    "    )\n",
    "    \n",
    "    if out.empty:\n",
    "        print(f\"No cells with data in both {earliest_month} and {latest_month}\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # DEBUG: Show coordinate distribution\n",
    "    unique_cells = out[[gx_col, gy_col]].drop_duplicates()\n",
    "    print(f\"DEBUG: {len(unique_cells)} unique grid cells\")\n",
    "    print(f\"DEBUG: {gx_col} range: {out[gx_col].min()} to {out[gx_col].max()}\")\n",
    "    print(f\"DEBUG: {gy_col} range: {out[gy_col].min()} to {out[gy_col].max()}\")\n",
    "    if len(unique_cells) <= 10:\n",
    "        print(f\"DEBUG: Cells - {unique_cells.values.tolist()}\")\n",
    "    \n",
    "    # Compute deltas\n",
    "    out[\"delta_gbp\"] = out[\"price_latest\"] - out[\"price_earliest\"]\n",
    "    out[\"delta_pct\"] = np.where(\n",
    "        out[\"price_earliest\"] > 0,\n",
    "        (out[\"price_latest\"] / out[\"price_earliest\"] - 1.0) * 100.0,\n",
    "        np.nan\n",
    "    )\n",
    "    \n",
    "    # Add metadata\n",
    "    out[cell_col] = out[gx_col].astype(\"int64\").astype(str) + \"_\" + out[gy_col].astype(\"int64\").astype(str)\n",
    "    out[\"end_month_earliest\"] = earliest_month\n",
    "    out[\"end_month_latest\"] = latest_month\n",
    "    out[\"years_delta\"] = (pd.to_datetime(latest_month).year - pd.to_datetime(earliest_month).year)\n",
    "    # Also expose generic coordinate columns and a generic cell id for JSON consumers\n",
    "    out[\"gx\"] = out[gx_col].astype(float)\n",
    "    out[\"gy\"] = out[gy_col].astype(float)\n",
    "    out[\"cell\"] = out[cell_col].astype(str)\n",
    "    \n",
    "    # Select output columns (clean up grid-specific columns)\n",
    "    # NOTE: sales_latest retained to contextualize large delta_pct values (sparse vs. robust moves)\n",
    "    keep_cols = [\n",
    "        gx_col, gy_col, \"gx\", \"gy\", cell_col, \"cell\", \"property_type\", \"new_build\",\n",
    "        \"price_earliest\", \"sales_earliest\", \"end_month_earliest\",\n",
    "        \"price_latest\", \"sales_latest\", \"end_month_latest\",\n",
    "        \"delta_gbp\", \"delta_pct\", \"years_delta\"\n",
    "    ]\n",
    "    out = out[[c for c in keep_cols if c in out.columns]]\n",
    "    \n",
    "    # Filter to non-null deltas only (saves storage)\n",
    "    out = out.dropna(subset=[\"delta_pct\"])\n",
    "    \n",
    "    # Convert Timestamps to ISO format strings for JSON serialization\n",
    "    out[\"end_month_earliest\"] = pd.to_datetime(out[\"end_month_earliest\"]).dt.strftime(\"%Y-%m-%d\")\n",
    "    out[\"end_month_latest\"] = pd.to_datetime(out[\"end_month_latest\"]).dt.strftime(\"%Y-%m-%d\")\n",
    "    \n",
    "    return out.sort_values(\"delta_pct\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5abe9eb",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "title": "[code]"
   },
   "outputs": [],
   "source": [
    "g = 5000\n",
    "grid_annual = grid_5km_annual\n",
    "\n",
    "delta_25 = build_delta_df(\n",
    "    grid_annual, g,\n",
    "    property_type=\"D\",\n",
    "    new_build=\"ALL\",\n",
    "    min_sales=30\n",
    ")\n",
    "\n",
    "plot_delta_map(delta_25, g, metric=\"delta_pct\", opacity=0.6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15995a3",
   "metadata": {},
   "source": [
    "## Computing Overall Deltas (Earliest → Latest)\n",
    "\n",
    "For Kaggle efficiency, use `build_overall_deltas()` to compute deltas across your entire dataset history.\n",
    "This single comparison is much cheaper than computing all period-to-period deltas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2b5fc7",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "title": "[code]"
   },
   "outputs": [],
   "source": [
    "# Example: Compute overall deltas for all property types across 5km, 10km, 25km grids\n",
    "# (Skip 1km to avoid excessive data volume)\n",
    "\n",
    "delta_metadata = {}\n",
    "\n",
    "for grid_size, grid_annual in [\n",
    "    (5000, grid_5km_annual),\n",
    "    (10000, grid_10km_annual),\n",
    "    (25000, grid_25km_annual),\n",
    "]:\n",
    "    grid_label = f\"{grid_size // 1000}km\"\n",
    "    overall_deltas = build_overall_deltas(grid_annual, min_sales=5)\n",
    "    \n",
    "    if not overall_deltas.empty:\n",
    "        cell_col = f\"cell_{grid_size}\"\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Overall Deltas: {grid_label}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        print(f\"Generated {len(overall_deltas)} cells with price movement\")\n",
    "        print(f\"Time range: {overall_deltas['end_month_earliest'].iloc[0]} → {overall_deltas['end_month_latest'].iloc[0]}\")\n",
    "        print(f\"Years: {overall_deltas['years_delta'].iloc[0]}\")\n",
    "        print(f\"\\nTop 10 gainers (%):\")\n",
    "        print(overall_deltas.nlargest(10, \"delta_pct\")[[cell_col, \"property_type\", \"new_build\", \"sales_latest\", \"delta_pct\", \"delta_gbp\"]])\n",
    "        \n",
    "        # Ensure grid coordinate columns are native Python types and add lon/lat center\n",
    "        gx_col = f\"gx_{grid_size}\"\n",
    "        gy_col = f\"gy_{grid_size}\"\n",
    "        try:\n",
    "            overall_deltas[gx_col] = overall_deltas[gx_col].astype(\"int64\")\n",
    "            overall_deltas[gy_col] = overall_deltas[gy_col].astype(\"int64\")\n",
    "        except Exception:\n",
    "            # best-effort cast\n",
    "            overall_deltas[gx_col] = overall_deltas[gx_col].astype(float).round(0).astype(\"Int64\")\n",
    "            overall_deltas[gy_col] = overall_deltas[gy_col].astype(float).round(0).astype(\"Int64\")\n",
    "\n",
    "        overall_deltas[\"gx\"] = overall_deltas[\"gx\"].astype(float).round(0).astype(\"Int64\")\n",
    "        overall_deltas[\"gy\"] = overall_deltas[\"gy\"].astype(float).round(0).astype(\"Int64\")\n",
    "\n",
    "        # Add centre lon/lat for convenience (EPSG:27700 -> EPSG:4326)\n",
    "        try:\n",
    "            x_cent = overall_deltas[gx_col].astype(float).to_numpy() + (grid_size / 2.0)\n",
    "            y_cent = overall_deltas[gy_col].astype(float).to_numpy() + (grid_size / 2.0)\n",
    "            lon_c, lat_c = TRANSFORMER.transform(x_cent, y_cent)\n",
    "            overall_deltas[\"lon\"] = lon_c\n",
    "            overall_deltas[\"lat\"] = lat_c\n",
    "        except Exception:\n",
    "            overall_deltas[\"lon\"] = None\n",
    "            overall_deltas[\"lat\"] = None\n",
    "\n",
    "        # Convert NaNs to None for JSON and ensure timestamps are strings\n",
    "        overall_deltas[\"end_month_earliest\"] = overall_deltas[\"end_month_earliest\"].astype(str)\n",
    "        overall_deltas[\"end_month_latest\"] = overall_deltas[\"end_month_latest\"].astype(str)\n",
    "\n",
    "        # Capture metadata for this grid\n",
    "        earliest = overall_deltas[\"end_month_earliest\"].iloc[0]\n",
    "        latest = overall_deltas[\"end_month_latest\"].iloc[0]\n",
    "        delta_metadata[grid_label] = {\n",
    "            \"earliest\": earliest,\n",
    "            \"latest\": latest,\n",
    "            \"rows\": len(overall_deltas)\n",
    "        }\n",
    "\n",
    "        # Save to JSON (memory-efficient for Kaggle)\n",
    "        output_path = f\"/kaggle/working/deltas_overall_{grid_label}.json.gz\"\n",
    "        with gzip.open(output_path, \"wt\", encoding=\"utf-8\") as f:\n",
    "            json.dump(overall_deltas.where(pd.notnull(overall_deltas), None).to_dict(orient=\"records\"), f)\n",
    "        print(f\"\\nSaved to {output_path}\")\n",
    "    else:\n",
    "        print(f\"\\nNo deltas generated for {grid_label}\")\n",
    "\n",
    "# Save delta metadata file\n",
    "if delta_metadata:\n",
    "    delta_metadata_path = \"/kaggle/working/deltas_metadata.json\"\n",
    "    with open(delta_metadata_path, \"w\") as f:\n",
    "        json.dump(delta_metadata, f, indent=2)\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"Delta metadata:\")\n",
    "    print(json.dumps(delta_metadata, indent=2))\n",
    "    print(f\"Saved to {delta_metadata_path}\")\n",
    "#plot_top_movers(delta_25, n=20)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "title,jupyter,execution,-all",
   "cell_metadata_json": true,
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
